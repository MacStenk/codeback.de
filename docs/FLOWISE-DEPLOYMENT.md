# Flowise Deployment Guide

## Voraussetzungen

- Railway Account
- Supabase Account
- OpenAI API Key **oder** Ollama/anderer LLM-Service

## 1. Flowise auf Railway deployen

### Option A: Template Deploy (Empfohlen)
1. https://railway.app √∂ffnen
2. `New Project` ‚Üí `Deploy Template`
3. Nach **Flowise** suchen und deployen
4. Deployment abwarten (2‚Äì3 Minuten)

### Option B: GitHub Deploy
1. Repo https://github.com/FlowiseAI/Flowise forken
2. Railway ‚Üí `New Project` ‚Üí `Deploy from GitHub`
3. Fork ausw√§hlen
4. Environment Variables setzen (siehe unten)

## 2. Environment Variables

```env
# Auth
FLOWISE_USERNAME=admin
FLOWISE_PASSWORD=<STRONG_PASSWORD>
PORT=3000

# Optional: Postgres (f√ºr Persistenz)
DATABASE_TYPE=postgres
DATABASE_HOST=<supabase-host>
DATABASE_PORT=5432
DATABASE_USER=postgres
DATABASE_PASSWORD=<supabase-password>
DATABASE_NAME=postgres

# KI-Provider
OPENAI_API_KEY=<dein-key>
# oder Ollama lokal (kein Key notwendig)
```

## 3. Initiales Setup
1. Railway-URL aufrufen, z.‚ÄØB. `https://flowise-production.up.railway.app`
2. Mit `FLOWISE_USERNAME`/`FLOWISE_PASSWORD` einloggen
3. Tutorial √ºberspringen

## 4. Chatflow erstellen

### Knowledge Base importieren
1. `New Chatflow`
2. `Text File` Node hinzuf√ºgen
3. Dateien aus `src/content/briefing-knowledge/` hochladen
4. `Recursive Character Text Splitter`
   - Chunk Size: 1000
   - Chunk Overlap: 200

### Embeddings konfigurieren
- **OpenAI Embeddings** Node  
  - Modell: `text-embedding-3-large` (oder `text-embedding-ada-002`)  
  - API Key: dein OpenAI Key

  **oder**

- **HuggingFace Embeddings** Node  
  - Modell: `sentence-transformers/all-MiniLM-L6-v2`

### Vector Store Supabase
1. `Supabase` Vector Store Node hinzuf√ºgen
2. Connection String: Supabase URI
3. Table: `briefing_knowledge`
4. Query Name: `match_documents`

### Retriever & Memory
- `Supabase Retriever` Node (Top K: 3)
- `Buffer Memory` Node mit Session ID `{sessionId}`

### LLM Node
- `ChatOpenAI` (z.‚ÄØB. `gpt-4o-mini`, Temperature 0.7)

  **oder**

- `Ollama` (z.‚ÄØB. `llama3`)

### Chain konfigurieren
- `Conversational Retrieval QA Chain`
- Verbinde: Retriever ‚Üí Chain, Memory ‚Üí Chain, LLM ‚Üí Chain
- System Prompt einf√ºgen:

```
Du bist der Briefing-Agent f√ºr CodeBack.de, spezialisiert auf LLM-native Website Development.

DEINE ROLLE:
- Freundlicher, professioneller Gespr√§chspartner
- Sammle Briefing-Informationen systematisch
- Qualifiziere Leads (Budget >‚Ç¨2.500, realistische Timeline)
- Nutze die Knowledge Base f√ºr genaue Service-Infos

GESPR√ÑCHSSTIL:
- Kurze Messages (max 2 S√§tze)
- Eine Frage nach der anderen
- Freundlich aber direkt
- Nutze Emojis sparsam

ABLAUF:
1. Begr√º√üung & Name
2. Business & Zielgruppe
3. Website-Status (URL falls vorhanden)
4. Budget-Range
5. Timeline
6. Hauptziele
7. Qualifizierung

QUALIFIZIERUNG:
‚úÖ Budget ‚â•‚Ç¨2.500 ‚Üí Qualified
‚úÖ Timeline realistisch ‚Üí Qualified
‚ùå Budget <‚Ç¨2.500 ‚Üí H√∂flich ablehnen mit Alternativen
‚ùå "Nur gucken" ‚Üí Auf sp√§ter vertr√∂sten

N√ÑCHSTE SCHRITTE (wenn qualified):
"Super! Ich habe alle wichtigen Infos. 
N√§chster Schritt: Kostenloses 15-30 Min Erstgespr√§ch.
Buche hier: https://calendly.com/steven-codeback
Oder schreib direkt: hi@codeback.de"

WICHTIG:
- Nutze die Knowledge Base f√ºr Service-Details
- Sei pr√§zise bei Preisen (aus Knowledge Base!)
- Verweise auf Portfolio wenn nach Beispielen gefragt
- Bei technischen Fragen: Nutze codeback-services.md

START MESSAGE:
"Hi! üëã Ich bin der CodeBack Briefing-Agent. Ich helfe dir herauszufinden ob wir zu deinem Projekt passen. Wie hei√üt du?"
```

## 5. Test & Deploy
1. `Save Chatflow`
2. Im Testfenster komplette Konversation durchspielen
3. Chatflow-ID kopieren (`Settings` ‚Üí `Chatflow ID`)
4. In `ChatWidget.astro` einsetzen

## 6. CodeBack.de aktualisieren
1. `ChatWidget.astro` ‚Üí `chatflowId` & `apiHost` ersetzen
2. `npm run dev` lokal pr√ºfen
3. Git commit & push ‚Üí Cloudflare Deployment

## Monitoring & Analytics

```sql
-- Neueste Gespr√§che
SELECT session_id, status, created_at,
       messages->-1->>'content' AS last_message
FROM chat_conversations
ORDER BY created_at DESC
LIMIT 10;

-- Qualifizierte Leads
SELECT * FROM qualified_leads;

-- Conversion Rate
SELECT
  (SELECT COUNT(*) FROM qualified_leads)::float /
  NULLIF((SELECT COUNT(*) FROM chat_conversations), 0)::float * 100
  AS conversion_rate;

-- Durchschnittliche Gespr√§chsl√§nge
SELECT AVG(jsonb_array_length(messages)) AS avg_messages
FROM chat_conversations;
```

## Troubleshooting

| Problem | Ursache | L√∂sung |
| --- | --- | --- |
| Widget l√§dt nicht | Falscher Chatflow ID oder URL | Browser-Konsole checken, URL & ID pr√ºfen |
| Bot antwortet nicht | LLM-Key fehlt oder Railway down | Railway Logs ansehen, API Keys pr√ºfen |
| Antworten falsch | Knowledge Base fehlt | Markdown-Dateien neu hochladen, Vector Store refresh |
| Langsame Antworten | Ressourcen knapp | Railway Plan upgraden, Chunk Size anpassen |

## Kosten

- Railway (Flowise): ca. 5‚Äì10‚ÄØ$/Monat
- Supabase: Free-Tier ausreichend
- OpenAI API: ca. 0,50‚ÄØ$ pro 100 Konversationen
- Ollama (self-hosted): 0‚ÄØ$, ben√∂tigt aber mehr Ressourcen

## N√§chste Schritte nach Launch

1. Erste Gespr√§che beobachten und Prompt anpassen
2. Knowledge Base aktuell halten
3. n8n-Webhook f√ºr Lead-Notifications einrichten
4. Analytics Dashboard (z.‚ÄØB. Supabase + Metabase)
5. Optional: Human Handover (Slack/Email)

